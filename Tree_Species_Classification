{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8937562,"sourceType":"datasetVersion","datasetId":5377367},{"sourceId":12490715,"sourceType":"datasetVersion","datasetId":7882304},{"sourceId":12519024,"sourceType":"datasetVersion","datasetId":7902296},{"sourceId":12519034,"sourceType":"datasetVersion","datasetId":7902304}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:14:34.743337Z","iopub.execute_input":"2025-07-22T18:14:34.743612Z","iopub.status.idle":"2025-07-22T18:14:34.747774Z","shell.execute_reply.started":"2025-07-22T18:14:34.743591Z","shell.execute_reply":"2025-07-22T18:14:34.747132Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import os\nimport shutil\n\nsrc = '/kaggle/input/tree-species-identification-dataset/Tree_Species_Dataset'\ndst = '/kaggle/working/Clean_Tree_Species_Dataset'\n\nos.makedirs(dst, exist_ok=True)\n\n# List and sort folders to ensure consistent indexing\nfolders = sorted([f for f in os.listdir(src) if os.path.isdir(os.path.join(src, f)) and not f.startswith('.')])\n\n# Copy all folders except the 20th one \nfor idx, folder in enumerate(folders):\n    if idx == 20:\n        print(f\"Skipping 20th class: {folder}\")\n        continue\n    full_path = os.path.join(src, folder)\n    shutil.copytree(full_path, os.path.join(dst, folder))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:14:35.040336Z","iopub.execute_input":"2025-07-22T18:14:35.040572Z","iopub.status.idle":"2025-07-22T18:14:35.105216Z","shell.execute_reply.started":"2025-07-22T18:14:35.040556Z","shell.execute_reply":"2025-07-22T18:14:35.104334Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/811310695.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    574\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mignored_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirs_exist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0muse_srcentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n","\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/kaggle/working/Clean_Tree_Species_Dataset/amla'"],"ename":"FileExistsError","evalue":"[Errno 17] File exists: '/kaggle/working/Clean_Tree_Species_Dataset/amla'","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"import cv2\nimport os\n\nfolder_path = \"/kaggle/working/Clean_Tree_Species_Dataset/babul\"\nimage_sizes = []\n\nfor filename in os.listdir(folder_path):\n    if filename.endswith(('.jpg', '.png', '.jpeg')):\n        img = cv2.imread(os.path.join(folder_path, filename))\n        h, w = img.shape[:2]\n        image_sizes.append((w, h))  # width, height\n\nprint(\"Unique sizes:\", set(image_sizes))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:30:46.662572Z","iopub.execute_input":"2025-07-22T18:30:46.662852Z","iopub.status.idle":"2025-07-22T18:30:46.937871Z","shell.execute_reply.started":"2025-07-22T18:30:46.662830Z","shell.execute_reply":"2025-07-22T18:30:46.937198Z"}},"outputs":[{"name":"stdout","text":"Unique sizes: {(260, 194), (4000, 3000), (225, 224), (768, 1024), (1300, 866), (512, 384), (400, 274), (276, 183), (249, 202), (300, 168), (1800, 1347), (259, 194), (1024, 680), (280, 180), (248, 203), (800, 600), (1024, 768), (210, 240), (194, 259), (512, 288), (283, 178), (225, 225), (241, 209), (262, 192), (275, 183), (245, 206), (400, 400)}\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"# Data Retrival","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image_dataset_from_directory\ntrain_ds,val_ds=image_dataset_from_directory(\n    '/kaggle/working/Clean_Tree_Species_Dataset',\n    image_size=(224,224),\n    batch_size=32,\n    labels='inferred',\n    label_mode='categorical',\n    validation_split=0.2,\n    subset='both',\n    shuffle='True',\n    seed=42,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:31:21.304731Z","iopub.execute_input":"2025-07-22T18:31:21.305385Z","iopub.status.idle":"2025-07-22T18:31:23.103710Z","shell.execute_reply.started":"2025-07-22T18:31:21.305357Z","shell.execute_reply":"2025-07-22T18:31:23.102703Z"}},"outputs":[{"name":"stdout","text":"Found 1450 files belonging to 29 classes.\nUsing 1160 files for training.\nUsing 290 files for validation.\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1753209081.469962      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10057 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"# help(ImageDataGenerator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:31:25.553452Z","iopub.execute_input":"2025-07-22T18:31:25.553954Z","iopub.status.idle":"2025-07-22T18:31:25.558106Z","shell.execute_reply.started":"2025-07-22T18:31:25.553928Z","shell.execute_reply":"2025-07-22T18:31:25.556720Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_augmen1=ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    rotation_range=40,\n    horizontal_flip=True,\n    brightness_range=[0.8,1.2],\n    zoom_range=0.3,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    fill_mode='reflect',\n    validation_split=0.2,\n)\ntest_augmen1=ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:31:25.714095Z","iopub.execute_input":"2025-07-22T18:31:25.714347Z","iopub.status.idle":"2025-07-22T18:31:25.719704Z","shell.execute_reply.started":"2025-07-22T18:31:25.714327Z","shell.execute_reply":"2025-07-22T18:31:25.718969Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"train_generator=train_augmen1.flow_from_directory(\n    '/kaggle/working/Clean_Tree_Species_Dataset',\n    target_size=(320,320),\n    batch_size=32,\n    class_mode='categorical',\n    subset='training',\n    seed=42\n)\nval_generator=test_augmen1.flow_from_directory(\n    '/kaggle/working/Clean_Tree_Species_Dataset',\n    target_size=(320,320),\n    batch_size=32,\n    class_mode='categorical',\n    subset='validation',\n    seed=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:31:25.825643Z","iopub.execute_input":"2025-07-22T18:31:25.825887Z","iopub.status.idle":"2025-07-22T18:31:25.880675Z","shell.execute_reply.started":"2025-07-22T18:31:25.825868Z","shell.execute_reply":"2025-07-22T18:31:25.880144Z"}},"outputs":[{"name":"stdout","text":"Found 1160 images belonging to 29 classes.\nFound 290 images belonging to 29 classes.\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"import numpy as np\nnp.unique(train_generator.classes, return_counts=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:31:28.563952Z","iopub.execute_input":"2025-07-22T18:31:28.564268Z","iopub.status.idle":"2025-07-22T18:31:28.570437Z","shell.execute_reply.started":"2025-07-22T18:31:28.564246Z","shell.execute_reply":"2025-07-22T18:31:28.569842Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28], dtype=int32),\n array([40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,\n        40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]))"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Fetch one batch\nimages, labels = next(train_generator)\n\n# Plot the first 10 images and their labels\nplt.figure(figsize=(16, 6))\nfor i in range(10):\n    img = images[i]\n    label = np.argmax(labels[i])  # Convert one-hot to class index\n    plt.subplot(2, 5, i + 1)\n    plt.imshow((img * 255).astype(\"uint8\"))\n    plt.axis('off')\n    plt.title(f\"Label: {label}\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Use Transfer Learning- \n1.EfficientNetB0 (high accuracy with few parameters)\n\n2.DenseNet121","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.applications import DenseNet121,DenseNet201\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:29:17.093596Z","iopub.execute_input":"2025-07-20T07:29:17.094116Z","iopub.status.idle":"2025-07-20T07:29:17.104331Z","shell.execute_reply.started":"2025-07-20T07:29:17.094096Z","shell.execute_reply":"2025-07-20T07:29:17.103638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model=DenseNet201(\n    weights='imagenet',\n    include_top=False,\n    input_shape=(320,320,3)\n)\nbase_model.model_trainable=True\nset_trainable=True\n\nfor layer in base_model.layers[:-90]:\n    layer.trainable=False\n    \n# for layer in base_model.layers:\n#     print(layer.name,layer.trainable)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:29:17.105088Z","iopub.execute_input":"2025-07-20T07:29:17.105281Z","iopub.status.idle":"2025-07-20T07:29:22.165138Z","shell.execute_reply.started":"2025-07-20T07:29:17.105264Z","shell.execute_reply":"2025-07-20T07:29:22.164562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model=Sequential([\n    base_model,\n    GlobalAveragePooling2D(),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(512,activation='relu'),\n    BatchNormalization(),\n    Dropout(0.3),\n    Dense(256,activation='relu'),\n    BatchNormalization(),\n    Dropout(0.2),\n    Dense(29,activation='softmax'),\n])\nmodel.compile(optimizer=Adam(0.001),metrics=['accuracy'],loss='categorical_crossentropy')\n\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:29:22.165823Z","iopub.execute_input":"2025-07-20T07:29:22.166024Z","iopub.status.idle":"2025-07-20T07:29:22.264812Z","shell.execute_reply.started":"2025-07-20T07:29:22.166009Z","shell.execute_reply":"2025-07-20T07:29:22.264092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\n\nearly = EarlyStopping(\n    monitor='val_loss', \n    min_delta=0.0001,\n    patience=5,             \n    restore_best_weights=True,  \n    verbose=1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:29:22.265574Z","iopub.execute_input":"2025-07-20T07:29:22.265780Z","iopub.status.idle":"2025-07-20T07:29:22.271039Z","shell.execute_reply.started":"2025-07-20T07:29:22.265756Z","shell.execute_reply":"2025-07-20T07:29:22.270505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history=model.fit(train_generator,validation_data=val_generator,epochs=50,batch_size=32,verbose=1,callbacks=[early])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:29:22.271656Z","iopub.execute_input":"2025-07-20T07:29:22.271928Z","iopub.status.idle":"2025-07-20T07:42:21.437328Z","shell.execute_reply.started":"2025-07-20T07:29:22.271895Z","shell.execute_reply":"2025-07-20T07:42:21.436784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['val_accuracy'])\nplt.plot(history.history['accuracy'])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:42:21.438333Z","iopub.execute_input":"2025-07-20T07:42:21.438580Z","iopub.status.idle":"2025-07-20T07:42:21.561161Z","shell.execute_reply.started":"2025-07-20T07:42:21.438555Z","shell.execute_reply":"2025-07-20T07:42:21.560304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nfrom IPython.display import display\npath='/kaggle/input/sugarcane/Cane.png.webp'\nimg=Image.open(path)\ndisplay(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:42:21.562013Z","iopub.execute_input":"2025-07-20T07:42:21.562229Z","iopub.status.idle":"2025-07-20T07:42:21.796489Z","shell.execute_reply.started":"2025-07-20T07:42:21.562213Z","shell.execute_reply":"2025-07-20T07:42:21.795498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nimg = image.load_img(path, target_size=(320, 320))\nimg_array = image.img_to_array(img)\nimg_array = img_array / 255.0\nimg_array = np.expand_dims(img_array, axis=0)\ny_pred=model.predict(img_array)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:42:21.797439Z","iopub.execute_input":"2025-07-20T07:42:21.797993Z","iopub.status.idle":"2025-07-20T07:42:45.889374Z","shell.execute_reply.started":"2025-07-20T07:42:21.797970Z","shell.execute_reply":"2025-07-20T07:42:45.888821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ans=np.argmax(y_pred)\nans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:42:45.890332Z","iopub.execute_input":"2025-07-20T07:42:45.890527Z","iopub.status.idle":"2025-07-20T07:42:45.895617Z","shell.execute_reply.started":"2025-07-20T07:42:45.890512Z","shell.execute_reply":"2025-07-20T07:42:45.894975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = list(train_generator.class_indices.keys())\nprint(\"Class names:\", class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:31:41.270213Z","iopub.execute_input":"2025-07-22T18:31:41.270487Z","iopub.status.idle":"2025-07-22T18:31:41.274592Z","shell.execute_reply.started":"2025-07-22T18:31:41.270465Z","shell.execute_reply":"2025-07-22T18:31:41.274005Z"}},"outputs":[{"name":"stdout","text":"Class names: ['amla', 'asopalav', 'babul', 'bamboo', 'banyan', 'bili', 'cactus', 'champa', 'coconut', 'garmalo', 'gulmohor', 'gunda', 'jamun', 'kanchan', 'kesudo', 'khajur', 'mango', 'motichanoti', 'neem', 'nilgiri', 'pilikaren', 'pipal', 'saptaparni', 'shirish', 'simlo', 'sitafal', 'sonmahor', 'sugarcane', 'vad']\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"for i in range(len(class_names)):\n    if i==ans:\n        print(class_names[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:42:45.914039Z","iopub.execute_input":"2025-07-20T07:42:45.914199Z","iopub.status.idle":"2025-07-20T07:42:45.932261Z","shell.execute_reply.started":"2025-07-20T07:42:45.914186Z","shell.execute_reply":"2025-07-20T07:42:45.931579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# use DenseNet33BL","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:42:45.933242Z","iopub.execute_input":"2025-07-20T07:42:45.933867Z","iopub.status.idle":"2025-07-20T07:42:45.952637Z","shell.execute_reply.started":"2025-07-20T07:42:45.933848Z","shell.execute_reply":"2025-07-20T07:42:45.952101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n1. use better keras model\n2.keras fine tuning using ,...\n3. Mix training ImageDataGenerator and  normal images for better accuracy\n4.Label Smooting\nCosine Annealing or OneCycleLR\nA dynamic LR schedule like CosineDecayRestarts could improve convergence.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:42:45.953464Z","iopub.execute_input":"2025-07-20T07:42:45.953819Z","iopub.status.idle":"2025-07-20T07:42:45.968683Z","shell.execute_reply.started":"2025-07-20T07:42:45.953796Z","shell.execute_reply":"2025-07-20T07:42:45.968049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vision Transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:42:45.969256Z","iopub.execute_input":"2025-07-20T07:42:45.969458Z","iopub.status.idle":"2025-07-20T07:42:45.982847Z","shell.execute_reply.started":"2025-07-20T07:42:45.969433Z","shell.execute_reply":"2025-07-20T07:42:45.982259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    model.save(\"tree_species_model.h5\")\n    print(\"Model saved successfully.\")\nexcept Exception as e:\n    print(\"Error:\", e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:01:21.625646Z","iopub.execute_input":"2025-07-20T08:01:21.626364Z","iopub.status.idle":"2025-07-20T08:01:22.878168Z","shell.execute_reply.started":"2025-07-20T08:01:21.626338Z","shell.execute_reply":"2025-07-20T08:01:22.877591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nmodel1 = load_model('model.h5')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:00:46.762252Z","iopub.execute_input":"2025-07-20T08:00:46.762835Z","iopub.status.idle":"2025-07-20T08:00:49.500833Z","shell.execute_reply.started":"2025-07-20T08:00:46.762812Z","shell.execute_reply":"2025-07-20T08:00:49.500044Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vision Transformers","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers timm\n\nimport os\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:33:42.031503Z","iopub.execute_input":"2025-07-22T17:33:42.031685Z","iopub.status.idle":"2025-07-22T17:35:17.225883Z","shell.execute_reply.started":"2025-07-22T17:33:42.031669Z","shell.execute_reply":"2025-07-22T17:35:17.225003Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"2025-07-22 17:35:04.049581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753205704.255594      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753205704.315463      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Directory path\ndata_dir = \"/kaggle/working/Clean_Tree_Species_Dataset\"\nimage_size = 224\nbatch_size = 32\n\n# Define transforms\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\nval_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(data_dir, transform=train_transform)\nclass_names = full_dataset.classes\nnum_classes = len(class_names)\n\n# Split into train and val\ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n# Update val transform separately\nval_dataset.dataset.transform = val_transform\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:16:32.839587Z","iopub.execute_input":"2025-07-22T18:16:32.840229Z","iopub.status.idle":"2025-07-22T18:16:32.853053Z","shell.execute_reply.started":"2025-07-22T18:16:32.840202Z","shell.execute_reply":"2025-07-22T18:16:32.852228Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224-in21k\",\n    num_labels=num_classes,\n    ignore_mismatched_sizes=True  # Very important if mismatch\n).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:16:32.854284Z","iopub.execute_input":"2025-07-22T18:16:32.854494Z","iopub.status.idle":"2025-07-22T18:16:33.296463Z","shell.execute_reply.started":"2025-07-22T18:16:32.854471Z","shell.execute_reply":"2025-07-22T18:16:33.295785Z"}},"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"optimizer = optim.AdamW(model.parameters(), lr=4e-5)\ncriterion = nn.CrossEntropyLoss()\n\ndef train_epoch(model, dataloader):\n    model.train()\n    total_loss, total_correct = 0, 0\n    for inputs, labels in tqdm(dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(pixel_values=inputs).logits\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_correct += (outputs.argmax(1) == labels).sum().item()\n    \n    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n\ndef evaluate(model, dataloader):\n    model.eval()\n    total_correct = 0\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(pixel_values=inputs).logits\n            total_correct += (outputs.argmax(1) == labels).sum().item()\n    return total_correct / len(dataloader.dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:16:33.297166Z","iopub.execute_input":"2025-07-22T18:16:33.297348Z","iopub.status.idle":"2025-07-22T18:16:33.307323Z","shell.execute_reply.started":"2025-07-22T18:16:33.297333Z","shell.execute_reply":"2025-07-22T18:16:33.306770Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"for epoch in range(30):  # you can increase to 10–20 later\n    # ---- Training ----\n    model.train()\n    train_loss, train_correct = 0, 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        outputs = model(pixel_values=images).logits\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        train_correct += (outputs.argmax(1) == labels).sum().item()\n    \n    train_acc = train_correct / len(train_loader.dataset)\n    avg_train_loss = train_loss / len(train_loader)\n\n    # ---- Validation ----\n    model.eval()\n    val_loss, val_correct = 0, 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(pixel_values=images).logits\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item()\n            val_correct += (outputs.argmax(1) == labels).sum().item()\n    \n    val_acc = val_correct / len(val_loader.dataset)\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), save_path)\n        print(f\"✅ Best model saved at epoch {epoch+1} with Val Acc: {val_acc:.4f}\")\n    avg_val_loss = val_loss / len(val_loader)\n    \n    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:16:33.308391Z","iopub.execute_input":"2025-07-22T18:16:33.308612Z","iopub.status.idle":"2025-07-22T18:28:38.414699Z","shell.execute_reply.started":"2025-07-22T18:16:33.308591Z","shell.execute_reply":"2025-07-22T18:28:38.413877Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 | Train Loss: 3.1665 | Train Acc: 0.3603 | Val Loss: 2.9633 | Val Acc: 0.6483\nEpoch 2 | Train Loss: 2.6247 | Train Acc: 0.8466 | Val Loss: 2.4777 | Val Acc: 0.8241\nEpoch 3 | Train Loss: 2.0954 | Train Acc: 0.9224 | Val Loss: 2.0570 | Val Acc: 0.8655\nEpoch 4 | Train Loss: 1.6691 | Train Acc: 0.9483 | Val Loss: 1.7434 | Val Acc: 0.8931\nEpoch 5 | Train Loss: 1.3256 | Train Acc: 0.9681 | Val Loss: 1.4710 | Val Acc: 0.9103\nEpoch 6 | Train Loss: 1.0520 | Train Acc: 0.9759 | Val Loss: 1.2728 | Val Acc: 0.9000\nEpoch 7 | Train Loss: 0.8457 | Train Acc: 0.9922 | Val Loss: 1.1381 | Val Acc: 0.9034\nEpoch 8 | Train Loss: 0.6886 | Train Acc: 0.9957 | Val Loss: 1.0381 | Val Acc: 0.9034\nEpoch 9 | Train Loss: 0.5716 | Train Acc: 0.9974 | Val Loss: 0.9556 | Val Acc: 0.9069\nEpoch 10 | Train Loss: 0.4774 | Train Acc: 0.9983 | Val Loss: 0.8774 | Val Acc: 0.9103\nEpoch 11 | Train Loss: 0.4061 | Train Acc: 0.9991 | Val Loss: 0.8366 | Val Acc: 0.9034\nEpoch 12 | Train Loss: 0.3505 | Train Acc: 0.9991 | Val Loss: 0.8000 | Val Acc: 0.8966\nEpoch 13 | Train Loss: 0.3074 | Train Acc: 0.9991 | Val Loss: 0.7647 | Val Acc: 0.9034\nEpoch 14 | Train Loss: 0.2715 | Train Acc: 1.0000 | Val Loss: 0.7548 | Val Acc: 0.8931\nEpoch 15 | Train Loss: 0.2429 | Train Acc: 1.0000 | Val Loss: 0.7352 | Val Acc: 0.8966\nEpoch 16 | Train Loss: 0.2188 | Train Acc: 1.0000 | Val Loss: 0.7091 | Val Acc: 0.8966\nEpoch 17 | Train Loss: 0.1971 | Train Acc: 1.0000 | Val Loss: 0.6961 | Val Acc: 0.8931\nEpoch 18 | Train Loss: 0.1793 | Train Acc: 1.0000 | Val Loss: 0.6807 | Val Acc: 0.8966\nEpoch 19 | Train Loss: 0.1635 | Train Acc: 1.0000 | Val Loss: 0.6698 | Val Acc: 0.8931\nEpoch 20 | Train Loss: 0.1498 | Train Acc: 1.0000 | Val Loss: 0.6591 | Val Acc: 0.8966\nEpoch 21 | Train Loss: 0.1382 | Train Acc: 1.0000 | Val Loss: 0.6523 | Val Acc: 0.8966\nEpoch 22 | Train Loss: 0.1276 | Train Acc: 1.0000 | Val Loss: 0.6409 | Val Acc: 0.8966\nEpoch 23 | Train Loss: 0.1181 | Train Acc: 1.0000 | Val Loss: 0.6353 | Val Acc: 0.8966\nEpoch 24 | Train Loss: 0.1099 | Train Acc: 1.0000 | Val Loss: 0.6332 | Val Acc: 0.8966\nEpoch 25 | Train Loss: 0.1021 | Train Acc: 1.0000 | Val Loss: 0.6295 | Val Acc: 0.8966\nEpoch 26 | Train Loss: 0.0951 | Train Acc: 1.0000 | Val Loss: 0.6254 | Val Acc: 0.8966\nEpoch 27 | Train Loss: 0.0889 | Train Acc: 1.0000 | Val Loss: 0.6193 | Val Acc: 0.8966\nEpoch 28 | Train Loss: 0.0833 | Train Acc: 1.0000 | Val Loss: 0.6180 | Val Acc: 0.8931\nEpoch 29 | Train Loss: 0.0782 | Train Acc: 1.0000 | Val Loss: 0.6140 | Val Acc: 0.8931\nEpoch 30 | Train Loss: 0.0734 | Train Acc: 1.0000 | Val Loss: 0.6139 | Val Acc: 0.8966\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}