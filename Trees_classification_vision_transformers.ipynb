{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPyCz5uNuMqsbMz5cRctiiW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya8215/Trees_classification/blob/main/Trees_classification_vision_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0k5YMGfvbaa"
      },
      "outputs": [],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Unzip dataset from Google Drive\n",
        "import zipfile, os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/zip_trees.zip'  # Update path if needed\n",
        "extract_path = '/content/Tree_Species_Dataset'"
      ],
      "metadata": {
        "id": "8m_rejX2viAb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "id": "jj0z_hFBvkYN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "src = '/content/Tree_Species_Dataset/Tree_Species_Dataset'\n",
        "dst = '/content/Updated_Tree_Species_Dataset'\n",
        "\n",
        "os.makedirs(dst, exist_ok=True)\n",
        "\n",
        "# List and sort folders to ensure consistent indexing\n",
        "folders = sorted([f for f in os.listdir(src) if os.path.isdir(os.path.join(src, f)) and not f.startswith('.')])\n",
        "\n",
        "# Copy all folders except the 20th one\n",
        "for idx, folder in enumerate(folders):\n",
        "    if idx == 20:\n",
        "        print(f\"Skipping 20th class: {folder}\")\n",
        "        continue\n",
        "    full_path = os.path.join(src, folder)\n",
        "    shutil.copytree(full_path, os.path.join(dst, folder))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OnQWB_WvlqJ",
        "outputId": "8910032c-a8d8-48cd-8e6e-8ee98cafc14c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping 20th class: other\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "train_ds,val_ds=image_dataset_from_directory(\n",
        "    '/content/Updated_Tree_Species_Dataset',\n",
        "    image_size=(224,224),\n",
        "    batch_size=32,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset='both',\n",
        "    shuffle='True',\n",
        "    seed=42,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBpwgA3bvoRd",
        "outputId": "96709869-2969-4a2c-9ba8-b1b33a971aff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1450 files belonging to 29 classes.\n",
            "Using 1160 files for training.\n",
            "Using 290 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers timm\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "8SmhlEXvvqIJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory path\n",
        "data_dir = \"/content/Updated_Tree_Species_Dataset\"\n",
        "image_size = 224\n",
        "batch_size = 32\n",
        "\n",
        "# Define transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# Load full dataset\n",
        "full_dataset = datasets.ImageFolder(data_dir, transform=train_transform)\n",
        "class_names = full_dataset.classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Split into train and val\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Update val transform separately\n",
        "val_dataset.dataset.transform = val_transform\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "1LpYVkiEv6du"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    num_labels=num_classes,\n",
        "    ignore_mismatched_sizes=True  # Very important if mismatch\n",
        ").to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO6_herOv-3S",
        "outputId": "0a2b96ec-bc1b-49c5-af4a-fec23df8fa30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=4e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_epoch(model, dataloader):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    for inputs, labels in tqdm(dataloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(pixel_values=inputs).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(pixel_values=inputs).logits\n",
        "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return total_correct / len(dataloader.dataset)\n"
      ],
      "metadata": {
        "id": "naMXiX-ewDWP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path='/content/model/'"
      ],
      "metadata": {
        "id": "NbsiZ8QpzuuE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(30):  # you can increase to 10–20 later\n",
        "    # ---- Training ----\n",
        "    model.train()\n",
        "    train_loss, train_correct = 0, 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(pixel_values=images).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    train_acc = train_correct / len(train_loader.dataset)\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # ---- Validation ----\n",
        "    model.eval()\n",
        "    best_val_acc=0\n",
        "    val_loss, val_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(pixel_values=images).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    val_acc = val_correct / len(val_loader.dataset)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), os.path.join(save_path, 'best_model.pth'))\n",
        "        print(f\"✅ Best model saved at epoch {epoch+1} with Val Acc: {val_acc:.4f}\")\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb1JCR40wFHB",
        "outputId": "dc6e19b4-b694-44f4-f489-18b10cd2a6d3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Best model saved at epoch 1 with Val Acc: 0.9034\n",
            "Epoch 1 | Train Loss: 1.0639 | Train Acc: 0.9871 | Val Loss: 1.3086 | Val Acc: 0.9034\n",
            "✅ Best model saved at epoch 2 with Val Acc: 0.9069\n",
            "Epoch 2 | Train Loss: 0.8518 | Train Acc: 0.9931 | Val Loss: 1.1491 | Val Acc: 0.9069\n",
            "✅ Best model saved at epoch 3 with Val Acc: 0.9138\n",
            "Epoch 3 | Train Loss: 0.6944 | Train Acc: 0.9948 | Val Loss: 1.0298 | Val Acc: 0.9138\n",
            "✅ Best model saved at epoch 4 with Val Acc: 0.8931\n",
            "Epoch 4 | Train Loss: 0.5675 | Train Acc: 0.9991 | Val Loss: 0.9498 | Val Acc: 0.8931\n",
            "✅ Best model saved at epoch 5 with Val Acc: 0.9034\n",
            "Epoch 5 | Train Loss: 0.4731 | Train Acc: 1.0000 | Val Loss: 0.8713 | Val Acc: 0.9034\n",
            "✅ Best model saved at epoch 6 with Val Acc: 0.9034\n",
            "Epoch 6 | Train Loss: 0.4023 | Train Acc: 1.0000 | Val Loss: 0.8068 | Val Acc: 0.9034\n",
            "✅ Best model saved at epoch 7 with Val Acc: 0.9034\n",
            "Epoch 7 | Train Loss: 0.3489 | Train Acc: 1.0000 | Val Loss: 0.7715 | Val Acc: 0.9034\n",
            "✅ Best model saved at epoch 8 with Val Acc: 0.8966\n",
            "Epoch 8 | Train Loss: 0.3060 | Train Acc: 1.0000 | Val Loss: 0.7271 | Val Acc: 0.8966\n",
            "✅ Best model saved at epoch 9 with Val Acc: 0.9000\n",
            "Epoch 9 | Train Loss: 0.2717 | Train Acc: 1.0000 | Val Loss: 0.6900 | Val Acc: 0.9000\n",
            "✅ Best model saved at epoch 10 with Val Acc: 0.9034\n",
            "Epoch 10 | Train Loss: 0.2428 | Train Acc: 1.0000 | Val Loss: 0.6637 | Val Acc: 0.9034\n",
            "✅ Best model saved at epoch 11 with Val Acc: 0.9000\n",
            "Epoch 11 | Train Loss: 0.2186 | Train Acc: 1.0000 | Val Loss: 0.6435 | Val Acc: 0.9000\n",
            "✅ Best model saved at epoch 12 with Val Acc: 0.9069\n",
            "Epoch 12 | Train Loss: 0.1982 | Train Acc: 1.0000 | Val Loss: 0.6189 | Val Acc: 0.9069\n",
            "✅ Best model saved at epoch 13 with Val Acc: 0.8966\n",
            "Epoch 13 | Train Loss: 0.1802 | Train Acc: 1.0000 | Val Loss: 0.6041 | Val Acc: 0.8966\n",
            "✅ Best model saved at epoch 14 with Val Acc: 0.9069\n",
            "Epoch 14 | Train Loss: 0.1645 | Train Acc: 1.0000 | Val Loss: 0.5856 | Val Acc: 0.9069\n",
            "✅ Best model saved at epoch 15 with Val Acc: 0.9000\n",
            "Epoch 15 | Train Loss: 0.1510 | Train Acc: 1.0000 | Val Loss: 0.5738 | Val Acc: 0.9000\n",
            "✅ Best model saved at epoch 16 with Val Acc: 0.9034\n",
            "Epoch 16 | Train Loss: 0.1388 | Train Acc: 1.0000 | Val Loss: 0.5580 | Val Acc: 0.9034\n",
            "✅ Best model saved at epoch 17 with Val Acc: 0.9069\n",
            "Epoch 17 | Train Loss: 0.1281 | Train Acc: 1.0000 | Val Loss: 0.5460 | Val Acc: 0.9069\n",
            "✅ Best model saved at epoch 18 with Val Acc: 0.9069\n",
            "Epoch 18 | Train Loss: 0.1189 | Train Acc: 1.0000 | Val Loss: 0.5357 | Val Acc: 0.9069\n",
            "✅ Best model saved at epoch 19 with Val Acc: 0.9069\n",
            "Epoch 19 | Train Loss: 0.1102 | Train Acc: 1.0000 | Val Loss: 0.5239 | Val Acc: 0.9069\n",
            "✅ Best model saved at epoch 20 with Val Acc: 0.9069\n",
            "Epoch 20 | Train Loss: 0.1026 | Train Acc: 1.0000 | Val Loss: 0.5139 | Val Acc: 0.9069\n",
            "✅ Best model saved at epoch 21 with Val Acc: 0.9103\n",
            "Epoch 21 | Train Loss: 0.0958 | Train Acc: 1.0000 | Val Loss: 0.5068 | Val Acc: 0.9103\n",
            "✅ Best model saved at epoch 22 with Val Acc: 0.9069\n",
            "Epoch 22 | Train Loss: 0.0894 | Train Acc: 1.0000 | Val Loss: 0.4994 | Val Acc: 0.9069\n",
            "✅ Best model saved at epoch 23 with Val Acc: 0.9103\n",
            "Epoch 23 | Train Loss: 0.0836 | Train Acc: 1.0000 | Val Loss: 0.4933 | Val Acc: 0.9103\n",
            "✅ Best model saved at epoch 24 with Val Acc: 0.9103\n",
            "Epoch 24 | Train Loss: 0.0784 | Train Acc: 1.0000 | Val Loss: 0.4867 | Val Acc: 0.9103\n",
            "✅ Best model saved at epoch 25 with Val Acc: 0.9069\n",
            "Epoch 25 | Train Loss: 0.0738 | Train Acc: 1.0000 | Val Loss: 0.4814 | Val Acc: 0.9069\n",
            "✅ Best model saved at epoch 26 with Val Acc: 0.9069\n",
            "Epoch 26 | Train Loss: 0.0693 | Train Acc: 1.0000 | Val Loss: 0.4751 | Val Acc: 0.9069\n",
            "✅ Best model saved at epoch 27 with Val Acc: 0.9069\n",
            "Epoch 27 | Train Loss: 0.0653 | Train Acc: 1.0000 | Val Loss: 0.4726 | Val Acc: 0.9069\n",
            "✅ Best model saved at epoch 28 with Val Acc: 0.9069\n",
            "Epoch 28 | Train Loss: 0.0618 | Train Acc: 1.0000 | Val Loss: 0.4668 | Val Acc: 0.9069\n",
            "✅ Best model saved at epoch 29 with Val Acc: 0.9103\n",
            "Epoch 29 | Train Loss: 0.0583 | Train Acc: 1.0000 | Val Loss: 0.4629 | Val Acc: 0.9103\n",
            "✅ Best model saved at epoch 30 with Val Acc: 0.9103\n",
            "Epoch 30 | Train Loss: 0.0551 | Train Acc: 1.0000 | Val Loss: 0.4593 | Val Acc: 0.9103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cqOEQ4u9wKfe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}